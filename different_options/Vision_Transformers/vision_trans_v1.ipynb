{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "REPO_DIR = \"/content/pushup-counter\"\n",
        "KINETICS_CSV = os.path.join(REPO_DIR, \"Kinetics_push_ups/kinetic_counts.csv\")\n",
        "KINETICS_DIR = os.path.join(REPO_DIR, \"Kinetics_push_ups/kinetics_pushups\")\n",
        "\n",
        "UCF_CSV = os.path.join(REPO_DIR, \"UCF101_push_ups/good_quality_ucf_counts.csv\")\n",
        "UCF_DIRS = [\n",
        "    os.path.join(REPO_DIR, \"UCF101_push_ups/ucf101_dataset/medium_quality\"),\n",
        "    os.path.join(REPO_DIR, \"UCF101_push_ups/ucf101_dataset/good_quality\")\n",
        "]\n",
        "\n",
        "video_extensions = (\".mp4\", \".avi\", \".mov\", \".mkv\")\n",
        "\n",
        "def find_missing_videos(csv_path, dirs):\n",
        "    df = pd.read_csv(csv_path)\n",
        "    df.columns = [c.strip() for c in df.columns]\n",
        "    video_col = next((c for c in df.columns if c.lower() in [\"video_id\",\"video\",\"filename\",\"file\",\"video_name\"]), None)\n",
        "    video_ids = df[video_col].astype(str).str.strip().tolist()\n",
        "\n",
        "    missing = []\n",
        "    for vid in video_ids:\n",
        "        found = False\n",
        "        for d in dirs:\n",
        "            for ext in [\"\"] + list(video_extensions):\n",
        "                path = os.path.join(d, vid if ext==\"\" else os.path.splitext(vid)[0]+ext)\n",
        "                if os.path.exists(path):\n",
        "                    found = True\n",
        "                    break\n",
        "            if found:\n",
        "                break\n",
        "        if not found:\n",
        "            missing.append(vid)\n",
        "    return missing\n",
        "\n",
        "# Check Kinetics\n",
        "missing_kinetics = find_missing_videos(KINETICS_CSV, [KINETICS_DIR])\n",
        "print(f\"Missing videos in Kinetics CSV: {len(missing_kinetics)}\")\n",
        "print(missing_kinetics)\n",
        "\n",
        "# # Check UCF\n",
        "# missing_ucf = find_missing_videos(UCF_CSV, UCF_DIRS)\n",
        "# print(f\"Missing videos in UCF CSV: {len(missing_ucf)}\")\n",
        "# print(missing_ucf)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "epaCkBT-1Q-r",
        "outputId": "5ee393eb-5e16-43f0-a381-6801d16c57dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing videos in Kinetics CSV: 0\n",
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import time\n",
        "import traceback\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from torchvision.models import vit_b_16\n"
      ],
      "metadata": {
        "id": "0fMjD5u7woJ3"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GitHub repo\n",
        "GITHUB_REPO = \"https://github.com/grace-pasion/pushup-counter\"\n",
        "\n",
        "# Local Colab paths\n",
        "REPO_DIR = \"/content/pushup-counter\"\n",
        "\n",
        "NUM_FRAMES = 32\n",
        "IMG_SIZE = 224\n",
        "BATCH_SIZE = 2\n",
        "NUM_EPOCHS = 8\n",
        "LEARNING_RATE = 0.001\n",
        "RANDOM_SEED = 42\n",
        "\n",
        "OUT_MODEL = \"vit_pushup_model_colab.pt\"\n",
        "OUT_PRED_CSV = \"vit_predictions_colab.csv\"\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", DEVICE)\n",
        "\n",
        "# ------------------------------\n",
        "# Clone GitHub Repo\n",
        "# ------------------------------\n",
        "if not os.path.exists(REPO_DIR):\n",
        "    !git clone --depth=1 {GITHUB_REPO} {REPO_DIR}\n",
        "\n",
        "# CSV paths\n",
        "UCF_CSV = os.path.join(REPO_DIR, \"UCF101_push_ups/good_quality_ucf_counts.csv\")\n",
        "UCF_VIDEO_DIRS = [\n",
        "    os.path.join(REPO_DIR, \"UCF101_push_ups/ucf101_dataset/medium_quality\"),\n",
        "    os.path.join(REPO_DIR, \"UCF101_push_ups/ucf101_dataset/good_quality\")\n",
        "]\n",
        "KINETICS_CSV = os.path.join(REPO_DIR, \"Kinetics_push_ups/kinetic_counts.csv\")\n",
        "KINETICS_VIDEO_DIR = os.path.join(REPO_DIR, \"Kinetics_push_ups/kinetics_pushups\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y8KoeBqEws_I",
        "outputId": "82492d2b-bc1f-445b-af3a-080b8050826b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Cloning into '/content/pushup-counter'...\n",
            "remote: Enumerating objects: 582, done.\u001b[K\n",
            "remote: Counting objects: 100% (582/582), done.\u001b[K\n",
            "remote: Compressing objects: 100% (578/578), done.\u001b[K\n",
            "remote: Total 582 (delta 4), reused 579 (delta 2), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (582/582), 868.34 MiB | 33.47 MiB/s, done.\n",
            "Resolving deltas: 100% (4/4), done.\n",
            "Updating files: 100% (569/569), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_csv_safe(path, dataset_name):\n",
        "    df = pd.read_csv(path, header=0)\n",
        "    df.columns = [c.strip() for c in df.columns]\n",
        "    vid_col = next((c for c in df.columns if c.lower() in [\"video_id\",\"video\",\"filename\",\"file\",\"video_name\"]), None)\n",
        "    count_col = next((c for c in df.columns if c.lower() in [\"count\",\"counts\",\"label_count\"]), None)\n",
        "    df = df.rename(columns={vid_col: \"video_id\", count_col: \"count\"})\n",
        "    df[\"video_id\"] = df[\"video_id\"].astype(str).str.strip()\n",
        "    df[\"count\"] = pd.to_numeric(df[\"count\"], errors=\"coerce\").fillna(0).astype(int)\n",
        "    df[\"dataset\"] = dataset_name\n",
        "    return df\n",
        "\n",
        "def load_all_data():\n",
        "    df_ucf = load_csv_safe(UCF_CSV, \"ucf\")\n",
        "    df_kin = load_csv_safe(KINETICS_CSV, \"kinetics\")\n",
        "    return pd.concat([df_ucf, df_kin], ignore_index=True)\n",
        "\n",
        "# ------------------------------\n",
        "# Find video path\n",
        "# ------------------------------\n",
        "def find_video_path(video_id, dataset):\n",
        "    search_dirs = UCF_VIDEO_DIRS if dataset==\"ucf\" else [KINETICS_VIDEO_DIR]\n",
        "    for d in search_dirs:\n",
        "        for ext in [\"\", \".mp4\", \".avi\", \".mov\", \".mkv\"]:\n",
        "            p = os.path.join(d, video_id if ext==\"\" else os.path.splitext(video_id)[0]+ext)\n",
        "            if os.path.exists(p):\n",
        "                return p\n",
        "    for d in search_dirs:\n",
        "        for root, _, files in os.walk(d):\n",
        "            for f in files:\n",
        "                if os.path.splitext(f)[0] == os.path.splitext(video_id)[0]:\n",
        "                    return os.path.join(root, f)\n",
        "    return None\n",
        "\n"
      ],
      "metadata": {
        "id": "dF7UrKwNxHv4"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ------------------------------\n",
        "# Dataset\n",
        "# ------------------------------\n",
        "class VideoFrameDataset(Dataset):\n",
        "    def __init__(self, df, img_size=IMG_SIZE, num_frames=NUM_FRAMES):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.img_size = img_size\n",
        "        self.num_frames = num_frames\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Resize((img_size,img_size)),\n",
        "            transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def _sample_frames(self, path):\n",
        "        cap = cv2.VideoCapture(path)\n",
        "        total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "        if total <= 0:\n",
        "            indices = [0]*self.num_frames\n",
        "        else:\n",
        "            indices = np.linspace(0, max(0,total-1), self.num_frames).astype(int)\n",
        "        frames = []\n",
        "        for idx in indices:\n",
        "            cap.set(cv2.CAP_PROP_POS_FRAMES, int(idx))\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                frame = np.zeros((self.img_size,self.img_size,3),dtype=np.uint8)\n",
        "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "            frames.append(frame)\n",
        "        cap.release()\n",
        "        return frames\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.loc[idx]\n",
        "        vid = row[\"video_id\"]\n",
        "        dataset = row[\"dataset\"]\n",
        "        true_count = float(row[\"count\"])\n",
        "        path = find_video_path(vid, dataset)\n",
        "        if path is None:\n",
        "            frames = [np.zeros((self.img_size,self.img_size,3),dtype=np.uint8) for _ in range(self.num_frames)]\n",
        "        else:\n",
        "            frames = self._sample_frames(path)\n",
        "        frames_tensor = torch.stack([self.transform(f) for f in frames])\n",
        "        return frames_tensor, torch.tensor([true_count],dtype=torch.float32), vid, path\n",
        "\n"
      ],
      "metadata": {
        "id": "nWd4-Z3uxeuL"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------\n",
        "# ViT Regression Model\n",
        "# ------------------------------\n",
        "class ViTPushupModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.vit = vit_b_16(weights='IMAGENET1K_V1')\n",
        "        self.vit.heads = nn.Identity()\n",
        "        self.reg_head = nn.Sequential(\n",
        "            nn.Linear(self.vit.hidden_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64,1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C,H,W = x.shape\n",
        "        x = x.view(B*T, C,H,W)\n",
        "        feats = self.vit(x)\n",
        "        feats = feats.view(B,T,-1)\n",
        "        feats = feats.mean(dim=1)\n",
        "        out = self.reg_head(feats)\n",
        "        return out\n",
        "\n"
      ],
      "metadata": {
        "id": "HQ4KK_zdxioR"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------\n",
        "# Training / Evaluation\n",
        "# ------------------------------\n",
        "def train_epoch(model, loader, optim, loss_fn, epoch):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    total_mae = 0\n",
        "    n = 0\n",
        "    pbar = tqdm(loader, desc=f\"Epoch {epoch} Training\", ncols=100)\n",
        "    for frames, counts, vids, paths in pbar:\n",
        "        frames = torch.stack(frames).to(DEVICE)\n",
        "        counts = torch.stack(counts).to(DEVICE)\n",
        "        optim.zero_grad()\n",
        "        preds = model(frames)\n",
        "        loss = loss_fn(preds, counts)\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "        total_loss += loss.item()*frames.size(0)\n",
        "        total_mae += torch.sum(torch.abs(preds-counts)).item()\n",
        "        n += frames.size(0)\n",
        "        pbar.set_postfix({\"loss\": total_loss/max(n,1), \"mae\": total_mae/max(n,1)})\n",
        "    return total_loss/n, total_mae/n\n",
        "\n",
        "def eval_model(model, loader, save_csv=None):\n",
        "    model.eval()\n",
        "    results = []\n",
        "    pbar = tqdm(loader, desc=\"Evaluating\", ncols=100)\n",
        "    with torch.no_grad():\n",
        "        for frames, counts, vids, paths in pbar:\n",
        "            frames = torch.stack(frames).to(DEVICE)\n",
        "            preds = model(frames).cpu().numpy().flatten()\n",
        "            counts_np = torch.stack(counts).numpy().flatten()\n",
        "            for vid,path,p,t in zip(vids, paths, preds, counts_np):\n",
        "                results.append({\"video_id\":vid,\"found_path\":path,\"true_count\":float(t),\"pred_count\":float(p)})\n",
        "    df = pd.DataFrame(results)\n",
        "    if save_csv:\n",
        "        df.to_csv(save_csv,index=False)\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "cRTpmzJBxk7N"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------\n",
        "# MAIN\n",
        "# ------------------------------\n",
        "def main():\n",
        "    torch.manual_seed(RANDOM_SEED)\n",
        "    np.random.seed(RANDOM_SEED)\n",
        "    random.seed(RANDOM_SEED)\n",
        "\n",
        "    df = load_all_data()\n",
        "    train_df, test_df = train_test_split(df, test_size=0.2, random_state=RANDOM_SEED, stratify=df[\"dataset\"])\n",
        "\n",
        "    train_ds = VideoFrameDataset(train_df)\n",
        "    test_ds = VideoFrameDataset(test_df)\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\n",
        "    test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))\n",
        "\n",
        "    model = ViTPushupModel().to(DEVICE)\n",
        "    loss_fn = nn.MSELoss()\n",
        "    optim = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "    best_val_mae = float(\"inf\")\n",
        "    for epoch in range(1, NUM_EPOCHS+1):\n",
        "        train_loss, train_mae = train_epoch(model, train_loader, optim, loss_fn, epoch)\n",
        "        val_df = eval_model(model, test_loader)\n",
        "        val_mae = float(np.mean(np.abs(val_df[\"pred_count\"]-val_df[\"true_count\"])))\n",
        "        print(f\"Epoch {epoch}: train_loss={train_loss:.3f}, train_mae={train_mae:.3f}, val_mae={val_mae:.3f}\")\n",
        "        if val_mae < best_val_mae:\n",
        "            best_val_mae = val_mae\n",
        "            torch.save(model.state_dict(), OUT_MODEL)\n",
        "            print(f\"Saved best model (val MAE={val_mae:.3f}) → {OUT_MODEL}\")\n",
        "\n",
        "    final_df = eval_model(model, test_loader, save_csv=OUT_PRED_CSV)\n",
        "    print(\"Final predictions saved:\", OUT_PRED_CSV)\n",
        "\n",
        "if __name__==\"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 634
        },
        "id": "AW-TdrKaxnEI",
        "outputId": "38908551-58ac-43c7-bade-88b5a9d160f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 Training: 100%|██████████████████████| 166/166 [48:03<00:00, 17.37s/it, loss=6.34, mae=1.85]\n",
            "Evaluating: 100%|███████████████████████████████████████████████████| 42/42 [09:49<00:00, 14.03s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: train_loss=6.343, train_mae=1.849, val_mae=2.148\n",
            "Saved best model (val MAE=2.148) → vit_pushup_model_colab.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2 Training: 100%|██████████████████████| 166/166 [48:12<00:00, 17.43s/it, loss=5.89, mae=1.77]\n",
            "Evaluating: 100%|███████████████████████████████████████████████████| 42/42 [09:43<00:00, 13.88s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2: train_loss=5.889, train_mae=1.770, val_mae=2.505\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3 Training: 100%|██████████████████████| 166/166 [48:04<00:00, 17.37s/it, loss=5.86, mae=1.78]\n",
            "Evaluating: 100%|███████████████████████████████████████████████████| 42/42 [09:46<00:00, 13.97s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3: train_loss=5.861, train_mae=1.779, val_mae=2.104\n",
            "Saved best model (val MAE=2.104) → vit_pushup_model_colab.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4 Training: 100%|██████████████████████| 166/166 [47:47<00:00, 17.27s/it, loss=5.86, mae=1.79]\n",
            "Evaluating: 100%|███████████████████████████████████████████████████| 42/42 [09:46<00:00, 13.96s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4: train_loss=5.863, train_mae=1.790, val_mae=2.207\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5 Training: 100%|██████████████████████| 166/166 [47:43<00:00, 17.25s/it, loss=5.85, mae=1.78]\n",
            "Evaluating: 100%|███████████████████████████████████████████████████| 42/42 [09:53<00:00, 14.12s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5: train_loss=5.850, train_mae=1.780, val_mae=2.169\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6 Training:   2%|▍                       | 3/166 [00:37<33:43, 12.42s/it, loss=4.37, mae=2.02]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-521378101.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-521378101.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mbest_val_mae\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"inf\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_mae\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mval_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mval_mae\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"pred_count\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mval_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"true_count\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-287819781.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model, loader, optim, loss_fn, epoch)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mpbar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"Epoch {epoch} Training\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mncols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mframes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcounts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpaths\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpbar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mcounts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcounts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    730\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 732\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    733\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    786\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    787\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 788\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    789\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2323779857.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_frames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sample_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0mframes_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mframes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mframes_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrue_count\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2323779857.py\u001b[0m in \u001b[0;36m_sample_frames\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m             \u001b[0mcap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCAP_PROP_POS_FRAMES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m             \u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#STOPPED IT EARLY: DOWNLOADING THE MODEL AND EVALUATION"
      ],
      "metadata": {
        "id": "QUE8xkeuTEL0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_PATH = \"/content/vit_pushup_model_colab (2).pt\"\n",
        "OUT_PRED_CSV = \"vit_predictions_from_saved_model.csv\"\n",
        "\n",
        "df = load_all_data()\n",
        "dataset = VideoFrameDataset(df)\n",
        "loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
        "                    collate_fn=lambda x: tuple(zip(*x)))\n",
        "\n",
        "model = ViTPushupModel().to(DEVICE)\n",
        "model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
        "print(\"Loaded model:\", MODEL_PATH)\n",
        "\n",
        "final_df = eval_model(model, loader, OUT_PRED_CSV)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nrGDXRCNTDxo",
        "outputId": "5dd54150-b3ac-454c-ee97-2fa16f3add39"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vit_b_16-c867db91.pth\" to /root/.cache/torch/hub/checkpoints/vit_b_16-c867db91.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 330M/330M [00:01<00:00, 195MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded model: /content/vit_pushup_model_colab (2).pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|█████████████████████████████████████████████████| 207/207 [45:10<00:00, 13.10s/it]\n"
          ]
        }
      ]
    }
  ]
}